import os
import json
import datetime

import streamlit as st
import ollama

try:
    OLLAMA_MODELS = ollama.list()["models"]
except Exception as e:
    st.warning("Please make sure Ollama is installed first. See https://ollama.ai for more details.")
    st.stop()

def st_ollama(model_name, user_question, chat_history_key, params):
    if chat_history_key not in st.session_state.keys():
        st.session_state[chat_history_key] = []

    print_chat_history_timeline(chat_history_key)
        
    if user_question:
        st.session_state[chat_history_key].append({"content": f"{user_question}", "role": "user"})
        with st.chat_message("question", avatar="ğŸ§‘â€ğŸš€"):
            st.write(user_question)

        # íŒŒë¼ë¯¸í„° ì¶œë ¥
        with st.chat_message("parameters", avatar="ğŸ”§"):
            st.write("Ollama Parameters:")
            for key, value in params.items():
                if key != "system":  # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ëŠ” ë³„ë„ë¡œ í‘œì‹œ
                    st.write(f"{key}: {value}")
            st.write(f"System Prompt: {params.get('system', 'None')}")

        messages = [dict(content=message["content"], role=message["role"]) for message in st.session_state[chat_history_key]]
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ì˜ ì‹œì‘ ë¶€ë¶„ì— ì¶”ê°€
        if params.get("system"):
            messages.insert(0, {"role": "system", "content": params["system"]})

        def llm_stream(response):
            response = ollama.chat(
                model_name, 
                messages, 
                stream=True,
                options={k: v for k, v in params.items() if k != "system"}  # system í”„ë¡¬í”„íŠ¸ëŠ” optionsì—ì„œ ì œì™¸
            )
            for chunk in response:
                yield chunk['message']['content']

        with st.chat_message("response", avatar="ğŸ¤–"):
            chat_box = st.empty()
            response_message = chat_box.write_stream(llm_stream(messages))

        st.session_state[chat_history_key].append({"content": f"{response_message}", "role": "assistant"})
        
        return response_message

def print_chat_history_timeline(chat_history_key):
    for message in st.session_state[chat_history_key]:
        role = message["role"]
        if role == "user":
            with st.chat_message("user", avatar="ğŸ§‘â€ğŸš€"): 
                question = message["content"]
                st.markdown(f"{question}", unsafe_allow_html=True)
        elif role == "assistant":
            with st.chat_message("assistant", avatar="ğŸ¤–"):
                st.markdown(message["content"], unsafe_allow_html=True)

def assert_models_installed():
    if len(OLLAMA_MODELS) < 1:
        st.sidebar.warning("No models found. Please install at least one model e.g. `ollama run llama2`")
        st.stop()

def select_model(key):
    model_names = ["ì„ íƒì•ˆí•¨"] + [model["name"] for model in OLLAMA_MODELS]
    default_index = model_names.index('gemma2:9b') if 'gemma2:9b' in model_names else 0
    llm_name = st.sidebar.selectbox(f"Choose Agent for {key}", model_names, index=default_index, key=f"model_select_{key}")
    if llm_name and llm_name != "ì„ íƒì•ˆí•¨":
        llm_details = [model for model in OLLAMA_MODELS if model["name"] == llm_name][0]
        if type(llm_details["size"]) != str:
            llm_details["size"] = f"{round(llm_details['size'] / 1e9, 2)} GB"
        with st.expander(f"LLM Details for {key}"):
            st.write(llm_details)
    return llm_name

def save_conversation(llm_name, conversation_key):
    OUTPUT_DIR = "llm_conversations"
    OUTPUT_DIR = os.path.join(os.getcwd(), OUTPUT_DIR)
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    filename = f"{OUTPUT_DIR}/{timestamp}_{llm_name.replace(':', '-')}"

    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)

    if st.session_state[conversation_key]:
        if st.sidebar.button(f"Save conversation {conversation_key}"):
            with open(f"{filename}_{conversation_key}.json", "w") as f:
                json.dump(st.session_state[conversation_key], f, indent=4)
            st.success(f"Conversation saved to {filename}_{conversation_key}.json")

if __name__ == "__main__":
    st.set_page_config(layout="wide", page_title="Ollama Chat", page_icon="ğŸ¦™")

    st.sidebar.title("Ollama Chat ğŸ¦™")
    
    assert_models_installed()

    # Model 1 ì„ íƒ
    llm_name_1 = select_model("Model 1")
    
    # Model 2 ì„ íƒ
    llm_name_2 = select_model("Model 2")

    # Model 1 Parameters
    # Model 1 Parameters
    params_1 = {}
    if llm_name_1 != "ì„ íƒì•ˆí•¨":
        st.sidebar.subheader("Model 1 Parameters")
        system_prompt_1 = st.sidebar.text_area("System Prompt for Model 1", "ì‚¬ìš©ìì˜ ë¬¸ì¥ì„ ì™„ì„±í•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ê²°ê³¼ë§Œ ì œê³µí•´ì£¼ì„¸ìš”.", help="ëª¨ë¸ì˜ ì „ë°˜ì ì¸ í–‰ë™ì„ ì§€ì‹œ. êµ¬ì²´ì ì¼ìˆ˜ë¡ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì–»ê¸° ì‰¬ì›€", height=100)
        params_1 = {
            "system": system_prompt_1,
            "num_predict": st.sidebar.number_input("Max Tokens (num_predict) 1", min_value=1, max_value=2048, value=1024, step=1, help="ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜. ë†’ì„ìˆ˜ë¡ ê¸´ ì‘ë‹µ, ë‚®ì„ìˆ˜ë¡ ì§§ì€ ì‘ë‹µ"),
            "temperature": st.sidebar.slider("Temperature 1", 0.0, 2.0, 0.0, 0.1, help="ë†’ì„ìˆ˜ë¡ ì°½ì˜ì„±ê³¼ ë‹¤ì–‘ì„± ì¦ê°€, ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„±ê³¼ ì •í™•ì„± ì¦ê°€"),
            "top_k": st.sidebar.slider("Top K 1", 1, 100, 40, 1, help="ë‹¤ìŒ í† í° ì„ íƒ ì‹œ ê³ ë ¤í•  ìƒìœ„ Kê°œì˜ í† í°. ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘ì„± ì¦ê°€, ë‚®ì„ìˆ˜ë¡ ì •í™•ì„± ì¦ê°€"),
            "top_p": st.sidebar.slider("Top P 1", 0.0, 1.0, 0.9, 0.05, help="ëˆ„ì  í™•ë¥  Pì— í•´ë‹¹í•˜ëŠ” ìƒìœ„ í† í°ë§Œ ê³ ë ¤. 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë‹¤ì–‘ì„± ì¦ê°€, 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì •í™•ì„± ì¦ê°€"),
            "repeat_penalty": st.sidebar.slider("Repeat Penalty 1", 0.0, 2.0, 1.1, 0.1, help="ë‹¨ì–´ ë°˜ë³µ ì–µì œ. ë†’ì„ìˆ˜ë¡ ë°˜ë³µ ê°ì†Œ, ë‚®ì„ìˆ˜ë¡ ìì—°ìŠ¤ëŸ¬ìš´ ë°˜ë³µ í—ˆìš©"),
            "presence_penalty": st.sidebar.slider("Presence Penalty 1", 0.0, 2.0, 0.0, 0.1, help="ìƒˆë¡œìš´ ì£¼ì œ ë„ì… ì¥ë ¤. ë†’ì„ìˆ˜ë¡ ìƒˆë¡œìš´ ì£¼ì œ ë„ì… ì¦ê°€, ë‚®ì„ìˆ˜ë¡ ê¸°ì¡´ ì£¼ì œ ìœ ì§€"),
            "frequency_penalty": st.sidebar.slider("Frequency Penalty 1", 0.0, 2.0, 0.0, 0.1, help="ìì£¼ ì‚¬ìš©ëœ ë‹¨ì–´ ì–µì œ. ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ ì–´íœ˜ ì‚¬ìš©, ë‚®ì„ìˆ˜ë¡ ìì—°ìŠ¤ëŸ¬ìš´ ë‹¨ì–´ ë¹ˆë„ ìœ ì§€"),
            "stop": st.sidebar.text_input("Stop Sequences 1 (comma-separated)", "", help="ìƒì„± ì¤‘ë‹¨ ì‹œí€€ìŠ¤. ì—¬ëŸ¬ ê°œ ì…ë ¥ ì‹œ ì‰¼í‘œë¡œ êµ¬ë¶„"),
            "seed": 1
        }
        params_1["stop"] = [s.strip() for s in params_1["stop"].split(',') if s.strip()]

    # Model 2 Parameters
    params_2 = {}
    if llm_name_2 != "ì„ íƒì•ˆí•¨":
        st.sidebar.subheader("Model 2 Parameters")
        system_prompt_2 = st.sidebar.text_area("System Prompt for Model 2", "ì‚¬ìš©ìì˜ ë¬¸ì¥ì„ ì™„ì„±í•˜ì„¸ìš”. ë°˜ë“œì‹œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ê²°ê³¼ë§Œ ì œê³µí•´ì£¼ì„¸ìš”.", help="ëª¨ë¸ì˜ ì „ë°˜ì ì¸ í–‰ë™ì„ ì§€ì‹œ. êµ¬ì²´ì ì¼ìˆ˜ë¡ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì–»ê¸° ì‰¬ì›€", height=100)
        params_2 = {
            "system": system_prompt_2,
            "num_predict": st.sidebar.number_input("Max Tokens (num_predict) 2", min_value=1, max_value=2048, value=1024, step=1, help="ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜. ë†’ì„ìˆ˜ë¡ ê¸´ ì‘ë‹µ, ë‚®ì„ìˆ˜ë¡ ì§§ì€ ì‘ë‹µ"),
            "temperature": st.sidebar.slider("Temperature 2", 0.0, 2.0, 2.0, 0.1, help="ë†’ì„ìˆ˜ë¡ ì°½ì˜ì„±ê³¼ ë‹¤ì–‘ì„± ì¦ê°€, ë‚®ì„ìˆ˜ë¡ ì¼ê´€ì„±ê³¼ ì •í™•ì„± ì¦ê°€"),
            "top_k": st.sidebar.slider("Top K 2", 1, 100, 40, 1, help="ë‹¤ìŒ í† í° ì„ íƒ ì‹œ ê³ ë ¤í•  ìƒìœ„ Kê°œì˜ í† í°. ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘ì„± ì¦ê°€, ë‚®ì„ìˆ˜ë¡ ì •í™•ì„± ì¦ê°€"),
            "top_p": st.sidebar.slider("Top P 2", 0.0, 1.0, 0.9, 0.05, help="ëˆ„ì  í™•ë¥  Pì— í•´ë‹¹í•˜ëŠ” ìƒìœ„ í† í°ë§Œ ê³ ë ¤. 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë‹¤ì–‘ì„± ì¦ê°€, 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì •í™•ì„± ì¦ê°€"),
            "repeat_penalty": st.sidebar.slider("Repeat Penalty 2", 0.0, 2.0, 1.1, 0.1, help="ë‹¨ì–´ ë°˜ë³µ ì–µì œ. ë†’ì„ìˆ˜ë¡ ë°˜ë³µ ê°ì†Œ, ë‚®ì„ìˆ˜ë¡ ìì—°ìŠ¤ëŸ¬ìš´ ë°˜ë³µ í—ˆìš©"),
            "presence_penalty": st.sidebar.slider("Presence Penalty 2", 0.0, 2.0, 0.0, 0.1, help="ìƒˆë¡œìš´ ì£¼ì œ ë„ì… ì¥ë ¤. ë†’ì„ìˆ˜ë¡ ìƒˆë¡œìš´ ì£¼ì œ ë„ì… ì¦ê°€, ë‚®ì„ìˆ˜ë¡ ê¸°ì¡´ ì£¼ì œ ìœ ì§€"),
            "frequency_penalty": st.sidebar.slider("Frequency Penalty 2", 0.0, 2.0, 0.0, 0.1, help="ìì£¼ ì‚¬ìš©ëœ ë‹¨ì–´ ì–µì œ. ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ ì–´íœ˜ ì‚¬ìš©, ë‚®ì„ìˆ˜ë¡ ìì—°ìŠ¤ëŸ¬ìš´ ë‹¨ì–´ ë¹ˆë„ ìœ ì§€"),
            "stop": st.sidebar.text_input("Stop Sequences 2 (comma-separated)", "", help="ìƒì„± ì¤‘ë‹¨ ì‹œí€€ìŠ¤. ì—¬ëŸ¬ ê°œ ì…ë ¥ ì‹œ ì‰¼í‘œë¡œ êµ¬ë¶„"),
            "seed": 41
        }
        params_2["stop"] = [s.strip() for s in params_2["stop"].split(',') if s.strip()]

    prompt = st.chat_input("Ask a question ...")

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("Model 1 Output")
        if llm_name_1 != "ì„ íƒì•ˆí•¨":
            conversation_key_1 = f"model_{llm_name_1}_1"
            st_ollama(llm_name_1, prompt, conversation_key_1, params_1)
            
            # Clear and Save buttons for Model 1
            if st.session_state.get(conversation_key_1):
                clear_conversation_1 = st.sidebar.button("Clear chat 1")
                if clear_conversation_1:
                    st.session_state[conversation_key_1] = []
                    st.rerun()
            save_conversation(llm_name_1, conversation_key_1)
        else:
            st.write("ëª¨ë¸ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    with col2:
        st.subheader("Model 2 Output")
        if llm_name_2 != "ì„ íƒì•ˆí•¨":
            conversation_key_2 = f"model_{llm_name_2}_2"
            st_ollama(llm_name_2, prompt, conversation_key_2, params_2)
            
            # Clear and Save buttons for Model 2
            if st.session_state.get(conversation_key_2):
                clear_conversation_2 = st.sidebar.button("Clear chat 2")
                if clear_conversation_2:
                    st.session_state[conversation_key_2] = []
                    st.rerun()
            save_conversation(llm_name_2, conversation_key_2)
        else:
            st.write("ëª¨ë¸ì´ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")